{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import uproot\n",
    "import pickle\n",
    "import nue_booster\n",
    "import importlib\n",
    "importlib.reload(nue_booster)\n",
    "\n",
    "import awkward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import localSettings as ls\n",
    "print(ls.ntuple_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARLOAD = [\"selected\", \"nu_pdg\", \"backtracked_pdg\",\"category\",\"npi0\",\n",
    "           \"shr_tkfit_dedx_U\",\"shr_tkfit_dedx_V\",\"shr_tkfit_dedx_Y\",\n",
    "           \"shr_tkfit_nhits_U\",\"shr_tkfit_nhits_V\",\"shr_tkfit_nhits_Y\",\n",
    "           \"trk_energy_tot\", \"shr_hits_tot\", \"ccnc\",\n",
    "           \"hits_ratio\", \"n_tracks_contained\", \n",
    "           \"NeutrinoEnergy2\",\n",
    "           \"CosmicIP\",\"CosmicDirAll3D\",\"CosmicIPAll3D\",\n",
    "           \"shrmoliereavg\",\"shrmoliererms\",\n",
    "           \"shr_tkfit_npointsvalid\",\"shr_tkfit_npoints\", # fitted vs. all hits for shower\n",
    "           \"shrclusfrac0\",\"shrclusfrac1\",\"shrclusfrac2\", # track-fitted hits / all hits\n",
    "           \"trkshrhitdist2\", # \"trkshrhitdist0\",\"trkshrhitdist1\", distance between track and shower in 2D\n",
    "           \"shrsubclusters0\",\"shrsubclusters1\",\"shrsubclusters2\", # number of sub-clusters in shower\n",
    "           \"trk_llr_pid_score_v\", # trk-PID score\n",
    "           \"trk_energy_proton_v\", # track energy under proton hyp\n",
    "           \"trk_calo_energy_y_v\", # track calo energy\n",
    "           \"reco_nu_vtx_sce_x\",\"reco_nu_vtx_sce_y\",\"reco_nu_vtx_sce_z\",\n",
    "           \"nu_e\", \"n_showers_contained\", \"shr_distance\", \"trk_distance\",\n",
    "           \"hits_y\", \"trk_len\", \"slnunhits\", \"slnhits\", \"shr_score\", \"trk_score\",\n",
    "           \"trk_energy\", \"tksh_distance\", \"tksh_angle\",\n",
    "           \"shr_energy_tot_cali\", \"evnunhits\", \"nslice\",\n",
    "           \"slclustfrac\", \"reco_nu_vtx_x\", \"reco_nu_vtx_y\", \"reco_nu_vtx_z\",\"contained_fraction\",\n",
    "           \"secondshower_Y_nhit\",\"secondshower_Y_vtxdist\",\"secondshower_Y_dot\",\"secondshower_Y_dir\",\"shrclusdir2\",\n",
    "           \"pfnhits\",\"pfnunhits\",\n",
    "           \"mcf_pass_ccpi0\",\"mcf_pass_ncpi0\",\"mcf_pass_ccnopi\",\"mcf_pass_ncnopi\",\"mcf_pass_cccpi\",\"mcf_pass_nccpi\"\n",
    "          ]\n",
    "WEIGHTS = [\"weightSpline\",\"weightTune\",\"weightSplineTimesTune\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = \"nuselection\"\n",
    "tree = \"NeutrinoSelectionFilter\"\n",
    "\n",
    "# train samples Run3\n",
    "NU3  = 'prodgenie_bnb_nu_uboone_overlay_mcc9.1_v08_00_00_26_filter_run3_reco2_G_reco2.root'\n",
    "NUEL3 = 'prestage_prodgenie_eLee_low_overlay_det_var_run3_mcc9.1_v08_00_00_26_cv_reco2.root'\n",
    "NUEH3 = 'prestage_prodgenie_eLee_high_overlay_det_var_run3_mcc9.1_v08_00_00_26_cv_reco2.root'\n",
    "NCPI0S3 = 'prodgenie_nc_pi0_uboone_overlay_reco2_v08_00_00_35_sameEXT_run3_reco2_G_reco2.root'\n",
    "CCPI03 = 'prodgenie_cc_pi0_uboone_overlay_v08_00_00_26_run3_G_reco2.root' #also a test sample: split!!!!\n",
    "CCNOPIT3 = 'prodgenie_tight_filter_CCmuNoPi_mcc9_v08_00_00_35_run3_reco2_reco2.root'\n",
    "NCNOPIT3 = 'prodgenie_tight_filter_NCNoPi_mcc9_v08_00_00_35_run3_reco2_reco2.root'\n",
    "\n",
    "# test samples Run3\n",
    "NUE3 = 'prodgenie_bnb_intrinsice_nue_uboone_overlay_mcc9.1_v08_00_00_26_run3_reco2_reco2.root'\n",
    "NCPI03  = 'prodgenie_nc_pi0_uboone_overlay_mcc9.1_v08_00_00_26_run3_G_reco2.root'\n",
    "CCNOPI3 = 'prodgenie_CCmuNoPi_overlay_mcc9_v08_00_00_33_all_run3_reco2_reco2.root'\n",
    "NCNOPI3 = 'prodgenie_ncnopi_overlay_mcc9_v08_00_00_33_new_run3_reco2_reco2.root'\n",
    "\n",
    "# train samples Run3\n",
    "NU1  = 'prodgenie_bnb_nu_uboone_overlay_mcc9.1_v08_00_00_26_filter_run1_reco2_reco2.root'\n",
    "NUEL1 = 'prestage_prodgenie_eLee_low_overlay_det_var_run1_mcc9.1_v08_00_00_26_cv_reco2.root'\n",
    "NUEH1 = 'prestage_prodgenie_eLee_high_overlay_det_var_run1_mcc9.1_v08_00_00_26_cv_reco2.root'\n",
    "NCPI0S1 = 'prodgenie_nc_pi0_uboone_overlay_reco2_v08_00_00_35_sameEXT_run1_reco2_reco2.root'\n",
    "CCPI0S1 = 'prodgenie_cc_pi0_uboone_overlay_supplementsameEXT_v08_00_00_26_run1_reco2.root'\n",
    "CCNOPIT1 = 'prodgenie_tight_filter_CCmuNoPi_mcc9_v08_00_00_35_run1_reco2_reco2.root'\n",
    "NCNOPIT1 = 'prodgenie_tight_filter_NCNoPi_mcc9_v08_00_00_35_run1_reco2_reco2.root'\n",
    "\n",
    "# test samples Run3\n",
    "NUE1 = 'prodgenie_bnb_intrinsice_nue_uboone_overlay_mcc9.1_v08_00_00_26_run1_reco2_reco2.root'\n",
    "NCPI01  = 'prodgenie_nc_pi0_uboone_overlay-v08_00_00_26_run1_reco2_reco2.root'\n",
    "CCPI01  = 'prodgenie_cc_pi0_uboone_overlay_v08_00_00_26_run1_reco2.root'\n",
    "CCNOPI1 = 'prodgenie_CCmuNoPi_overlay_mcc9_v08_00_00_33_all_run1_reco2_reco2.root'\n",
    "NCNOPI1 = 'prodgenie_ncnopi_overlay_mcc9_v08_00_00_33_run1_reco2_reco2.root'\n",
    "\n",
    "# train sample numi EXT\n",
    "EXTNUMI = 'numi_neutrinoselection_run1_beamoff_set0.root'\n",
    "EXTNUMI2 = 'neutrinoselection_filt_run1_NuMI_ext.root'\n",
    "#EXT = 'data_extbnb_mcc9.1_v08_00_00_25_reco2_D1_D2_E1_E2_all_reco2.root'\n",
    "\n",
    "#ntuple_path = ls.ntuple_path\n",
    "ntuple_path = \"/Users/cerati/Notebooks/PELEE/root_files/0304/\"\n",
    "\n",
    "u_mc3 = uproot.open(ntuple_path+NU3)[fold][tree]\n",
    "u_nuel3 = uproot.open(ntuple_path+NUEL3)[fold][tree]\n",
    "u_nueh3 = uproot.open(ntuple_path+NUEH3)[fold][tree]\n",
    "u_ncpi0s3 = uproot.open(ntuple_path+NCPI0S3)[fold][tree]\n",
    "u_ccpi03 = uproot.open(ntuple_path+CCPI03)[fold][tree]\n",
    "u_ccnopit3 = uproot.open(ntuple_path+CCNOPIT3)[fold][tree]\n",
    "u_ncnopit3 = uproot.open(ntuple_path+NCNOPIT3)[fold][tree]\n",
    "\n",
    "u_nue3 = uproot.open(ntuple_path+NUE3)[fold][tree]\n",
    "u_ncpi03 = uproot.open(ntuple_path+NCPI03)[fold][tree]\n",
    "u_ccnopi3 = uproot.open(ntuple_path+CCNOPI3)[fold][tree]\n",
    "u_ncnopi3 = uproot.open(ntuple_path+NCNOPI3)[fold][tree]\n",
    "\n",
    "u_mc1 = uproot.open(ntuple_path+NU1)[fold][tree]\n",
    "u_nuel1 = uproot.open(ntuple_path+NUEL1)[fold][tree]\n",
    "u_nueh1 = uproot.open(ntuple_path+NUEH1)[fold][tree]\n",
    "u_ncpi0s1 = uproot.open(ntuple_path+NCPI0S1)[fold][tree]\n",
    "u_ccpi0s1 = uproot.open(ntuple_path+CCPI0S1)[fold][tree]\n",
    "u_ccnopit1 = uproot.open(ntuple_path+CCNOPIT1)[fold][tree]\n",
    "u_ncnopit1 = uproot.open(ntuple_path+NCNOPIT1)[fold][tree]\n",
    "\n",
    "u_nue1 = uproot.open(ntuple_path+NUE1)[fold][tree]\n",
    "u_ncpi01 = uproot.open(ntuple_path+NCPI01)[fold][tree]\n",
    "u_ccpi01 = uproot.open(ntuple_path+CCPI01)[fold][tree]\n",
    "u_ccnopi1 = uproot.open(ntuple_path+CCNOPI1)[fold][tree]\n",
    "u_ncnopi1 = uproot.open(ntuple_path+NCNOPI1)[fold][tree]\n",
    "\n",
    "u_ext_numi = uproot.open(ntuple_path+EXTNUMI)[fold][tree]\n",
    "u_ext_numi2 = uproot.open(ntuple_path+EXTNUMI2)[fold][tree]\n",
    "#u_ext = uproot.open(ntuple_path+EXT)[fold][tree]\n",
    "\n",
    "uproot_v = [u_mc3,u_nue3,u_nuel3,u_nueh3,u_ncpi0s3,u_ccpi03,u_ccnopit3,u_ncnopit3,u_ncpi03,u_ccnopi3,u_ncnopi3,\n",
    "            u_mc1,u_nue1,u_nuel1,u_nueh1,u_ncpi0s1,u_ccpi0s1,u_ccnopit1,u_ncnopit1,u_ncpi01,u_ccpi01,u_ccnopi1,u_ncnopi1,\n",
    "            u_ext_numi,u_ext_numi2]#,u_ext]\n",
    "    \n",
    "mc3 = u_mc3.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "nuel3 = u_nuel3.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "nueh3 = u_nueh3.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ncpi0s3 = u_ncpi0s3.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ccpi03 = u_ccpi03.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ccnopit3 = u_ccnopit3.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ncnopit3 = u_ncnopit3.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "\n",
    "nue3 = u_nue3.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ncpi03 = u_ncpi03.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ccnopi3 = u_ccnopi3.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ncnopi3 = u_ncnopi3.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "\n",
    "mc1 = u_mc1.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "nuel1 = u_nuel1.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "nueh1 = u_nueh1.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ncpi0s1 = u_ncpi0s1.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ccpi0s1 = u_ccpi0s1.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ccnopit1 = u_ccnopit1.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ncnopit1 = u_ncnopit1.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "\n",
    "nue1 = u_nue1.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ncpi01 = u_ncpi01.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ccpi01 = u_ccpi01.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ccnopi1 = u_ccnopi1.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "ncnopi1 = u_ncnopi1.pandas.df(VARLOAD + WEIGHTS, flatten=False)\n",
    "\n",
    "ext_numi = u_ext_numi.pandas.df(VARLOAD, flatten=False)\n",
    "ext_numi2 = u_ext_numi2.pandas.df(VARLOAD, flatten=False)\n",
    "#ext = u_ext.pandas.df(VARLOAD, flatten=False)\n",
    "\n",
    "df_v = [mc3,nue3,nuel3,nueh3,ncpi0s3,ccpi03,ccnopit3,ncnopit3,ncpi03,ccnopi3,ncnopi3,\n",
    "        mc1,nue1,nuel1,nueh1,ncpi0s1,ccpi0s1,ccnopit1,ncnopit1,ncpi01,ccpi01,ccnopi1,ncnopi1,\n",
    "        ext_numi,ext_numi2]#,ext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to get the LLR-PID value for the \"track candidate\" (proton for nue selection, muon for numu)\n",
    "# can be done for any variable\n",
    "# code from Giuseppe!\n",
    "\n",
    "for i,df in enumerate(df_v):\n",
    "    up = uproot_v[i]\n",
    "    trk_llr_pid_v = up.array('trk_llr_pid_score_v')\n",
    "    trk_energy_proton_v = up.array('trk_energy_proton_v')\n",
    "    trk_calo_energy_y_v = up.array('trk_calo_energy_y_v')\n",
    "    trk_id = up.array('trk_id')-1 # I think we need this -1 to get the right result\n",
    "    trk_llr_pid_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_llr_pid_v,trk_id)])\n",
    "    trk_energy_proton_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_energy_proton_v,trk_id)])\n",
    "    trk_calo_energy_y_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_calo_energy_y_v,trk_id)])\n",
    "    df['trkpid'] = trk_llr_pid_v_sel\n",
    "    df['protonenergy'] = trk_energy_proton_sel\n",
    "    df['trackcaloenergy'] = trk_calo_energy_y_sel\n",
    "    df['subcluster'] = df['shrsubclusters0'] + df['shrsubclusters1'] + df['shrsubclusters2']\n",
    "    df['trkfit'] = df['shr_tkfit_npointsvalid'] / df['shr_tkfit_npoints']\n",
    "    df['anglediff_Y'] = np.abs(df['secondshower_Y_dir']-df['shrclusdir2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_noext = [mc3,nue3,nuel3,nueh3,ncpi0s3,ccpi03,ccnopit3,ncnopit3,ncpi03,ccnopi3,ncnopi3,\n",
    "              mc1,nue1,nuel1,nueh1,ncpi0s1,ccpi0s1,ccnopit1,ncnopit1,ncpi01,ccpi01,ccnopi1,ncnopi1]\n",
    "\n",
    "for i,df in enumerate(df_v_noext):\n",
    "    df.loc[ df['weightTune'] <= 0, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] == np.inf, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] > 100, 'weightTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightTune']) == True, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] <= 0, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] == np.inf, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] > 100, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightSplineTimesTune']) == True, 'weightSplineTimesTune' ] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(df_v):\n",
    "    df['shr_tkfit_nhits_tot'] = (df['shr_tkfit_nhits_Y']+df['shr_tkfit_nhits_U']+df['shr_tkfit_nhits_V'])\n",
    "    df['shr_tkfit_dedx_avg'] = (df['shr_tkfit_nhits_Y']*df['shr_tkfit_dedx_Y'] + df['shr_tkfit_nhits_U']*df['shr_tkfit_dedx_U'] + df['shr_tkfit_nhits_V']*df['shr_tkfit_dedx_V'])/df['shr_tkfit_nhits_tot']\n",
    "    df.loc[:,'shr_tkfit_dedx_max'] = df['shr_tkfit_dedx_Y']\n",
    "    df.loc[(df['shr_tkfit_nhits_U']>df['shr_tkfit_nhits_Y']),'shr_tkfit_dedx_max'] = df['shr_tkfit_dedx_U']\n",
    "    df.loc[(df['shr_tkfit_nhits_V']>df['shr_tkfit_nhits_Y']) & (df['shr_tkfit_nhits_V']>df['shr_tkfit_nhits_U']),'shr_tkfit_dedx_max'] = df['shr_tkfit_dedx_V']\n",
    "\n",
    "INTERCEPT = 0.0\n",
    "SLOPE = 0.83\n",
    "# define some energy-related variables\n",
    "for i,df in enumerate(df_v):\n",
    "    df[\"reco_e\"] = (df[\"shr_energy_tot_cali\"] + INTERCEPT) / SLOPE + df[\"trk_energy_tot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back the cosmic category\n",
    "for i,df in enumerate(df_v):\n",
    "    df.loc[(df['category']!=1)&(df['category']!=10)&(df['category']!=11)&(df['category']!=111)&(df['slnunhits']/df['slnhits']<0.2), 'category'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(df_v):\n",
    "    df[\"is_signal\"] = 0\n",
    "    df.loc[ df[\"category\"] == 10, 'is_signal' ] = 1\n",
    "    df.loc[ df[\"category\"] == 11, 'is_signal' ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCQUERY = '((abs(nu_pdg)==12 & ccnc==0) | mcf_pass_ccpi0==1 | mcf_pass_ncpi0==1 | mcf_pass_ccnopi==1 | mcf_pass_ncnopi==1 | mcf_pass_cccpi==1 | mcf_pass_nccpi==1)'\n",
    "test_mc3 = mc3.query('~'+MCQUERY)\n",
    "test_mc1 = mc1.query('~'+MCQUERY)\n",
    "\n",
    "mc3 = mc3.query(MCQUERY)\n",
    "mc1 = mc1.query(MCQUERY)\n",
    "\n",
    "train_ccpi03, test_ccpi03 = train_test_split(ccpi03, test_size=0.5, random_state=1990)\n",
    "train_ext_numi, test_ext_numi = train_test_split(ext_numi, test_size=0.05, random_state=1990)\n",
    "#train_ext_bnb, test_ext_bnb = train_test_split(ext, test_size=0.5, random_state=1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mc = pd.concat([mc3,mc1],ignore_index=True)\n",
    "train_nue = pd.concat([nuel3,nueh3,nuel1,nueh1],ignore_index=True)\n",
    "train_ncpi0 = pd.concat([ncpi0s3,ncpi0s1],ignore_index=True)\n",
    "train_ccpi0 = pd.concat([train_ccpi03,ccpi0s1],ignore_index=True)\n",
    "train_ccnopi = pd.concat([ccnopit3,ccnopit1],ignore_index=True)\n",
    "train_ncnopi = pd.concat([ncnopit3,ncnopit1],ignore_index=True)\n",
    "\n",
    "test_mc = pd.concat([test_mc3,test_mc1],ignore_index=True)\n",
    "test_nue = pd.concat([nue3,nue1],ignore_index=True)\n",
    "test_ncpi0 = pd.concat([ncpi03,ncpi01],ignore_index=True)\n",
    "test_ccpi0 = pd.concat([test_ccpi03,ccpi01],ignore_index=True)\n",
    "test_ccnopi = pd.concat([ccnopi3,ccnopi1],ignore_index=True)\n",
    "test_ncnopi = pd.concat([ncnopi3,ncnopi1],ignore_index=True)\n",
    "\n",
    "#train_ext = pd.concat([train_ext_bnb,train_ext_numi],ignore_index=True)\n",
    "#test_ext = pd.concat([test_ext_bnb,test_ext_numi],ignore_index=True)\n",
    "train_ext = pd.concat([train_ext_numi,ext_numi2],ignore_index=True)\n",
    "test_ext = test_ext_numi\n",
    "\n",
    "samples = {\n",
    "    \"mc\": (train_mc, test_mc),\n",
    "    \"nue\": (train_nue, test_nue),\n",
    "    \"ncpi0\": (train_ncpi0, test_ncpi0),\n",
    "    \"ccpi0\": (train_ccpi0, test_ccpi0),\n",
    "    \"ccnopi\": (train_ccnopi, test_ccnopi),\n",
    "    \"ncnopi\": (train_ncnopi, test_ncnopi),\n",
    "    \"ext\": (train_ext, test_ext),\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, df in samples.items():\n",
    "    df[0].loc[:,\"train_weight\"] = 1.\n",
    "    df[1].loc[:,\"train_weight\"] = 1.\n",
    "\n",
    "# override here train_weight for specific samples, if needed\n",
    "samples['ccnopi'][0].loc[:,\"train_weight\"] = 5.\n",
    "samples['ccnopi'][1].loc[:,\"train_weight\"] = 5.\n",
    "samples['ext'][0].loc[:,\"train_weight\"] = 10.\n",
    "samples['ext'][1].loc[:,\"train_weight\"] = 10.\n",
    "\n",
    "for k, df in samples.items():\n",
    "    df[0].loc[df[0]['category']==4,\"train_weight\"] = 10.\n",
    "    df[1].loc[df[1]['category']==4,\"train_weight\"] = 10.\n",
    "\n",
    "# set train_weight based on reco_e binning\n",
    "reco_bins = np.linspace(0.15,1.55,15)\n",
    "#reco_scaling = [8,5,3,3,2,2,2,1,1,1,1,1,1,1]\n",
    "reco_scaling = [1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "for k, df in samples.items():\n",
    "    for i, reco_bin in enumerate(reco_bins):\n",
    "        if i == 0: continue\n",
    "        df[0].loc[(df[0]['reco_e'] > reco_bins[i-1]) & (df[0]['reco_e'] < reco_bins[i]), 'train_weight'] = df[0]['train_weight']*reco_scaling[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables to be trained on\n",
    "TRAINVAR = [\"shr_score\",\"tksh_distance\",\"tksh_angle\",\n",
    "            \"shr_tkfit_dedx_max\",\n",
    "            \"trkfit\",\"trkpid\",\n",
    "            \"subcluster\",\"shrmoliereavg\",\n",
    "            \"trkshrhitdist2\",\"hits_ratio\",\n",
    "            \"secondshower_Y_nhit\",\"secondshower_Y_vtxdist\",\"secondshower_Y_dot\",\"anglediff_Y\",\n",
    "            \"CosmicIPAll3D\",\"CosmicDirAll3D\",\n",
    "            \"is_signal\",\"train_weight\",\"nu_e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (ls.pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "importlib.reload(nue_booster)\n",
    "my_booster = nue_booster.NueBooster(samples, TRAINVAR, random_state=1990)\n",
    "\n",
    "print (my_booster.variables)\n",
    "\n",
    "PRESEL = \"reco_e < 0.8\"\n",
    "PRESEL += ' and nslice == 1'\n",
    "PRESEL += ' and selected == 1'\n",
    "PRESEL += ' and shr_energy_tot_cali > 0.07'\n",
    "PRESEL += ' and n_tracks_contained > 0'\n",
    "PRESEL += ' and n_showers_contained == 1'\n",
    "\n",
    "my_booster.set_preselection(PRESEL)\n",
    "\n",
    "gain_imp = {}\n",
    "\n",
    "for label, bkg_query in zip(nue_booster.labels, nue_booster.bkg_queries):\n",
    "    \n",
    "    preds, gain_imp[label], evals_result = my_booster.train_booster(ax, bkg_query)\n",
    "    \n",
    "    with open(ls.pickle_path+'booster_%s_0304_extnumi_vx_test.pickle' % label, 'wb') as booster_file:\n",
    "        pickle.dump(preds, booster_file)\n",
    "\n",
    "    #print(evals_result)\n",
    "    epochs = len(evals_result['train']['error'])\n",
    "    x_axis = range(0, epochs)\n",
    "    # plot log loss\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.plot(x_axis, evals_result['train']['logloss'], label='Train')\n",
    "    ax1.plot(x_axis, evals_result['eval']['logloss'], label='Test')\n",
    "    ax1.legend()\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.title('XGBoost Log Loss')\n",
    "    plt.show()\n",
    "    # plot classification error\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    ax2.plot(x_axis, evals_result['train']['error'], label='Train')\n",
    "    ax2.plot(x_axis, evals_result['eval']['error'], label='Test')\n",
    "    ax2.legend()\n",
    "    plt.ylabel('Classification Error')\n",
    "    plt.title('XGBoost Classification Error')\n",
    "    plt.show()\n",
    "    # plot auc\n",
    "    fig3, ax3 = plt.subplots()\n",
    "    ax3.plot(x_axis, evals_result['train']['auc'], label='Train')\n",
    "    ax3.plot(x_axis, evals_result['eval']['auc'], label='Test')\n",
    "    ax3.legend()\n",
    "    plt.ylabel('Area Under Curve')\n",
    "    plt.title('XGBoost Area Under Curve')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.set_xlim([0, 1.0])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC')\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(ls.plots_path+\"roc_single.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mc',train_mc.query(PRESEL)[\"train_weight\"].sum())\n",
    "print('nue',train_nue.query(PRESEL)[\"train_weight\"].sum())\n",
    "print('ncpi0',train_ncpi0.query(PRESEL)[\"train_weight\"].sum())\n",
    "print('ccpi0',train_ccpi0.query(PRESEL)[\"train_weight\"].sum())\n",
    "print('ccnopi',train_ccnopi.query(PRESEL)[\"train_weight\"].sum())\n",
    "print('ncnopi',train_ncnopi.query(PRESEL)[\"train_weight\"].sum())\n",
    "print('ext',train_ext.query(PRESEL)[\"train_weight\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mc',test_mc.query(PRESEL)[\"train_weight\"].sum())\n",
    "print('nue',test_nue.query(PRESEL)[\"train_weight\"].sum())\n",
    "print('ncpi0',test_ncpi0.query(PRESEL)[\"train_weight\"].sum())\n",
    "print('ccpi0',test_ccpi0.query(PRESEL)[\"train_weight\"].sum())\n",
    "print('ccnopi',test_ccnopi.query(PRESEL)[\"train_weight\"].sum())\n",
    "print('ncnopi',test_ncnopi.query(PRESEL)[\"train_weight\"].sum())\n",
    "print('ext',test_ext.query(PRESEL)[\"train_weight\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gain_imp['pi0'])\n",
    "print(gain_imp['nonpi0'])\n",
    "\n",
    "if 'shr_tkfit_gap10_dedx_V' in gain_imp['pi0']:\n",
    "    gain_imp['pi0']['shr_tkfit_gap10_dedx_ALL'] = gain_imp['pi0']['shr_tkfit_gap10_dedx_V']+gain_imp['pi0']['shr_tkfit_gap10_dedx_U']+gain_imp['pi0']['shr_tkfit_gap10_dedx_Y']\n",
    "    gain_imp['nonpi0']['shr_tkfit_gap10_dedx_ALL'] = gain_imp['nonpi0']['shr_tkfit_gap10_dedx_V']+gain_imp['nonpi0']['shr_tkfit_gap10_dedx_U']+gain_imp['nonpi0']['shr_tkfit_gap10_dedx_Y']\n",
    "    del gain_imp['pi0']['shr_tkfit_gap10_dedx_V']\n",
    "    del gain_imp['pi0']['shr_tkfit_gap10_dedx_U']\n",
    "    del gain_imp['pi0']['shr_tkfit_gap10_dedx_Y']\n",
    "    del gain_imp['nonpi0']['shr_tkfit_gap10_dedx_V']\n",
    "    del gain_imp['nonpi0']['shr_tkfit_gap10_dedx_U']\n",
    "    del gain_imp['nonpi0']['shr_tkfit_gap10_dedx_Y']\n",
    "if 'shr_tkfit_2cm_dedx_V' in gain_imp['pi0']:\n",
    "    gain_imp['pi0']['shr_tkfit_2cm_dedx_ALL'] = gain_imp['pi0']['shr_tkfit_2cm_dedx_V']+gain_imp['pi0']['shr_tkfit_2cm_dedx_U']+gain_imp['pi0']['shr_tkfit_2cm_dedx_Y']\n",
    "    gain_imp['nonpi0']['shr_tkfit_2cm_dedx_ALL'] = gain_imp['nonpi0']['shr_tkfit_2cm_dedx_V']+gain_imp['nonpi0']['shr_tkfit_2cm_dedx_U']+gain_imp['nonpi0']['shr_tkfit_2cm_dedx_Y']\n",
    "    del gain_imp['pi0']['shr_tkfit_2cm_dedx_V']\n",
    "    del gain_imp['pi0']['shr_tkfit_2cm_dedx_U']\n",
    "    del gain_imp['pi0']['shr_tkfit_2cm_dedx_Y']\n",
    "    del gain_imp['nonpi0']['shr_tkfit_2cm_dedx_V']\n",
    "    del gain_imp['nonpi0']['shr_tkfit_2cm_dedx_U']\n",
    "    del gain_imp['nonpi0']['shr_tkfit_2cm_dedx_Y']\n",
    "\n",
    "labels = []\n",
    "pi0_imp = []\n",
    "nonpi0_imp = []\n",
    "\n",
    "for i in sorted (gain_imp['pi0'].keys()) :  \n",
    "    labels.append(i)\n",
    "    pi0_imp.append(gain_imp['pi0'][i])\n",
    "    nonpi0_imp.append(gain_imp['nonpi0'][i])\n",
    "    \n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "rects1 = ax.bar(x - width/2, pi0_imp, width, label='pi0')\n",
    "rects2 = ax.bar(x + width/2, nonpi0_imp, width, label='nonpi0')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('total gain')\n",
    "ax.set_title('BDT Variable Importance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation = 90)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(ls.plots_path+\"0109/bdt_var_gain.pdf\")\n",
    "#fig.savefig(ls.plots_path+\"bdt_var_gain.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_pi0_rnk = {}\n",
    "rnk = len(labels)\n",
    "for w in sorted(gain_imp['pi0'], key=gain_imp['pi0'].get, reverse=True):\n",
    "    d_pi0_rnk[w] = rnk\n",
    "    rnk = rnk-1\n",
    "print(d_pi0_rnk)\n",
    "\n",
    "d_nonpi0_rnk = {}\n",
    "rnk = len(labels)\n",
    "for w in sorted(gain_imp['nonpi0'], key=gain_imp['nonpi0'].get, reverse=True):\n",
    "    d_nonpi0_rnk[w] = rnk\n",
    "    rnk = rnk-1\n",
    "print(d_nonpi0_rnk)\n",
    "\n",
    "pi0_rnk = []\n",
    "nonpi0_rnk = []\n",
    "\n",
    "for i in labels:  \n",
    "    pi0_rnk.append(d_pi0_rnk[i])\n",
    "    nonpi0_rnk.append(d_nonpi0_rnk[i])\n",
    "    \n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "rects1 = ax.bar(x - width/2, pi0_rnk, width, label='pi0')\n",
    "rects2 = ax.bar(x + width/2, nonpi0_rnk, width, label='nonpi0')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Rank (based on total gain)')\n",
    "ax.set_title('BDT Variable Importance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation = 90)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(ls.plots_path+\"0109/bdt_var_rank.pdf\")\n",
    "#fig.savefig(ls.plots_path+\"bdt_var_rank.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
